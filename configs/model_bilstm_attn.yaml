model:
  name: "bilstm_attn"
  type: "rnn"
  
  # BiLSTM+Attention specific config
  input_size: 1629  # 543 landmarks * 3 (x,y,z)
  hidden_size: 512
  num_layers: 2
  dropout: 0.1
  bidirectional: true
  
  # Attention mechanism
  attention_type: "dot"  # dot, general, concat
  attention_hidden_size: 256
  
  # Classification head
  num_classes: 60  # BdSLW60 classes
  classifier_dropout: 0.1

training:
  learning_rate: 1e-3
  weight_decay: 1e-4
  batch_size: 32
  num_epochs: 100
  warmup_epochs: 5
  gradient_clip_norm: 1.0
  
  # Data augmentation
  temporal_dropout: 0.1
  noise_std: 0.01
  
  # Optimizer
  optimizer: "adam"
  betas: [0.9, 0.999]
  eps: 1e-8
  
  # Scheduler
  scheduler: "step"
  step_size: 30
  gamma: 0.1

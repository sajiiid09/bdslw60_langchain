model:
  name: "stformer_from_codes"
  type: "transformer"
  
  # ST-Former specific config
  patch_size: 16
  num_frames: 8
  hidden_size: 256
  num_hidden_layers: 4
  num_attention_heads: 4
  intermediate_size: 1024
  hidden_act: "gelu"
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  initializer_range: 0.02
  layer_norm_eps: 1e-12
  
  # Spatial-Temporal attention
  spatial_attention: true
  temporal_attention: true
  cross_attention: true
  
  # Input processing
  input_channels: 3
  image_size: 224
  
  # Classification head
  num_classes: 60
  classifier_dropout: 0.1

training:
  learning_rate: 1e-4
  weight_decay: 0.05
  batch_size: 4
  num_epochs: 1
  warmup_epochs: 0
  gradient_clip_norm: 1.0
  
  # Data augmentation
  mixup_alpha: 0.0
  cutmix_alpha: 0.0
  label_smoothing: 0.0
  
  # Optimizer
  optimizer: "adamw"
  betas: [0.9, 0.999]
  eps: 1e-8
  
  # Scheduler
  scheduler: "cosine"
  min_lr: 1e-6

